# Modelling in R using the Tody modelling workflow

[https://www.tmwr.org](https://www.tmwr.org)

1.1 Why use / what are models?

- Mathmatical tools to capture relationships in data
- Reduce complex relationships into simpler, human readable, terms
- Ease of use and sound scientific methodlogy should underlie all software used for modelling

***

1.2 Types of models

- **Descriptive** 
    - Describe or illustrate characteristics of some data - e.g. a LOESS regression model to detect trend in house prices
- **Inferential**
    - Used to test scientific hypotheses and produces statistical conclusions - e.g. clincal trial comapring drug efficacies
    - Produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability
    - Quaility depends on formal probablitic assumptions made about the data
- **Predictive** - designed to produce as accurate a prediction as possible about something - e.g. how many copies of book X will I sell?
    - **Mechanistic model** - build model equation from first principles depending on assumptions
        - Data used to estimate unknown params of equation so predictions can be generated
    - **Empirically driven** - created with more vague assumtions - fall into ML catagory
        - e.g. KNN model - Given ref data a new sample is predicted based on values of K most similar data
        - No theoretical or probabilistic assumptions made about data
        - Evaulation of appropriateness of model is assessed through accuracy using existing data

***

1.3 CONNECTIONS BETWEEN TYPES OF MODELS

- An ordinary linear regression model might fall into any of these three classes of model, depending on how it is used
- If a model has limited fidelity to the data, the inferences generated by the model should be highly suspect. In other words, statistical significance may not be sufficient proof that a model is appropriate

***

1.4 Terminology

- **Unsupervised model**
    - Don't have an outcome varaible
    - Learn patterns, clusters, characteristics e.g. PCA
    - Used to understand relationships between variables or sets of variables without an explicit relationship 
    between predictors and an outcome
 - **Supervised model**
    - Have outcome varaible
    - Linear regression / Neural networks etc.
    - Two broad sub-catagories (not exhaustive):
        - Regression predicts a numeric outcome
        - Classification predicts an outcome that is an ordered or unordered set of qualitative values
- **Outcomes**
    - Value being predicted in supervised models (aka labels / endpoints / dependent variables)
- **Independent variables**
    - Used to make predictions of the outcome (aks predictors, features, or covariates)

***

1.5 HOW DOES MODELING FIT INTO THE DATA ANALYSIS PROCESS?

Things to think about in advance of analyses:

- Clean your data
- Understand data (aka exploratory data analysis [EDA])
- Set good perfomnce metric (classification accuracy, true and false positive rates, root mean squared error etc.)

Modelling is an iterative process:

- **EDA** - to and fro between numerical analyses and data visualiation / discoveries lead to questions / understanding increases
- **Feature engineering** - Understanding gained from EDA results in more accurate model prediction
- **Model tuning and selection** - Variety of models generated and compared - paramteres are tweaked / tuned
- **Model evaluation** - assess the model’s performance metrics, examine residual plots, and conduct other EDA-like analyses to understand how well the models work

***

2 TIDYVERSE 

The desgn priciples of the tidyverse packages is to make software, functions and their syntax intuitive such that their operations are easily understood as the majority of R users
will not be developers of computer scientists.

- Functions should avoid returning a novel data structure. If the results are conducive to an existing data structure

***

3.1 R modelling fundamentals in baseR


```R
# Chapter 3.1 - Modelling in R using base functions

library(tidyverse)
data(crickets, package = "modeldata")
names(crickets)

# Plot the temperature on the x-axis, the chirp rate on the y-axis. The plot
# elements will be colored differently for each species:
ggplot(crickets, 
       aes(x = temp, y = rate, color = species, pch = species, lty = species)) + 
  # Plot points for each data point and color by species
  geom_point(size = 2) + 
  # Show a simple linear model fit created separately for each species:
  geom_smooth(method = lm, se = FALSE, alpha = 0.5) + 
  scale_color_brewer(palette = "Paired") +
  labs(x = "Temperature (C)", y = "Chirp Rate (per minute)")

# Run LM
# Formula is symbolic outcome variable on LHS (y-axis)
lm(rate ~ temp, crickets)

# Temperature and time added as separate main effects to the model
# A main effect is a model term that contains a single predictor variable.
lm(rate ~ temp + time, crickets) # No time in df though

# As species is non-numeric varaible most functions would choke on this
# However as species has 2 poss values function assigns 1 and 0
# To deal with catagorical varaiables dummy (numeric) varaibles required
# N - 1 Binary columns are required to parse catagorial variables
lm(rate ~ temp + species, crickets) # No time in df though

# Adding intereatcion terms are done by any of following
lm(rate ~ temp + species + temp:species, crickets) 
lm(rate ~ (temp + species) ^ 2, crickets)
lm(rate ~ temp * species, crickets)

# Matchmatical operations can be used
# Literal math can also be applied to the predictors using the 
# identity function I() to convert C to F
lm(rate ~ log(temp), crickets)
lm(rate ~ I( (temp * 9/5) + 32 ), crickets)

# Period represents all main effects for all cols 
# ^ 3 adds all two- and three-variable interactions to the model
lm(rate ~ (.) ^ 3, crickets)

## Diagnostic plots. -----
# Residual plots reasonable enough to conduct inferential analysis
interaction_fit <-  lm(rate ~ (temp + species)^2, data = crickets)

# Place two plots next to one another:
par(mfrow = c(1, 2))

# Show residuals vs predicted values:
plot(interaction_fit, which = 1)

# A normal quantile plot on the residuals:
plot(interaction_fit, which = 2)

## Is interaction term necessary - use ANOVA?
# Recompute the model without the interaction term and use the anova() method.
# Fit a reduced model:
main_effect_fit <-  lm(rate ~ temp + species, data = crickets) 

# Compare the two:
# p-value of 0.25 implies lack of evidence against the null that interaction 
# term is not needed by the model. Thus opt for model without interaction.
# The reassess residual plots to make sure that our theoretical assumptions are valid (they are!)
anova(main_effect_fit, interaction_fit)

# Inspect the coefficients, SEss, and p-values of each model term
summary(main_effect_fit)

## CONCLUSION

# The chirp rate for each species increases by 3.6 chirps as the temperature increases 
# by a single degree. This term shows strong statistical significance as evidenced by the 
# p-value. The species term has a value of -10.07. This indicates that, across all temperature 
# values, O. niveus has a chirp rate that is about 10 fewer chirps per minute than O. 
# exclamation is. Similar to the temperature term, the species effect is associated with a 
# very small p-value.
# 
# The only issue in this analysis is the intercept value. It indicates that at 0° C, there 
# are negative chirps per minute for both species. While this doesn’t make sense, the data 
# only go as low as 17.2° C and interpreting the model at 0° C would be an extrapolation. 
# This would be a bad idea. That being said, the model fit is good within the applicable 
# range of the temperature values; the conclusions should be limited to the observed 
# temperature range.

# If we need to predict value no in data use predict
new_values <- data.frame(species = "O. exclamationis", temp = 15:20)
predict(main_effect_fit, new_values)
```

3.2 What does formula does

3.3 Why tidyness is important for modelling

- Many functions in R to do the same thing in different ways need to be careful 
  incostitancies can be a stubling block
- Missing data is handled inconsitenatly
    - General rule is that missing data propagate more missing data; the average of a set of  values with a missing data point is itself missing but not always
    - Watch for `na.action()` function. Common policies are `na.fail()` and `na.omit()`

The broom package is useful for standardising the structure of R models into dataframes
ready for plotting:

```R
library(broom)
corr_res <- map(mtcars %>% select(-mpg), cor.test, y = mtcars$mpg)
corr_res %>% 
  # Convert each to a tidy format; `map_dfr()` stacks the data frames 
  map_dfr(tidy, .id = "predictor") %>% 
  ggplot(aes(x = fct_reorder(predictor, estimate))) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  labs(x = NULL, y = "Correlation with mpg")
```

3.4 Combining Base R models and the tidyverse

`dplyr::group_nest()` fits separte models for each cricket species and the
`data` col containe the rate and temp cols fro crickets in a list col. 
`purrr:map` is used to create individual models for each species.

```R
model_by_species <- 
  crickets %>%
  group_nest(species) %>%
  mutate(model = purrr::map(data, ~ lm(rate ~ temp, data = .x)))
model_by_species
```

`brrom::tidy()` is used to convert the coeffcients of the models into a
consistent data frame format so that they can be unnested:

```R
model_by_species %>% 
  mutate(coef = map(model, tidy)) %>% 
  select(species, coef) %>% 
  unnest(cols = c(coef))
#> # A tibble: 4 × 6
#>   species          term        estimate std.error statistic  p.value
#>   <fct>            <chr>          <dbl>     <dbl>     <dbl>    <dbl>
#> 1 O. exclamationis (Intercept)   -11.0      4.77      -2.32 3.90e- 2
#> 2 O. exclamationis temp            3.75     0.184     20.4  1.10e-10
#> 3 O. niveus        (Intercept)   -15.4      2.35      -6.56 9.07e- 6
#> 4 O. niveus        temp            3.52     0.105     33.6  1.57e-15
```

3.5 THE TIDYMODELS METAPACKAGE

The tidymodels package is a suite of packges for data modelling following a 
similar design to tidyverse.

***

4 The Ames Housing Data

GOAL: predict the sale price of a house based on other information we have, 
such as its characteristics and location.

```R
data(ames, package = "modeldata")
dim(ames)
library(tidymodels); theme_set(theme_bw()) # Theme set for ggplot2

ggplot(ames, aes(x = Sale_Price)) + 
  geom_histogram(bins = 50, col= "white")
```

- Data right skewed/ Median sale price was $160K / max sale price $755K
- Strong argument can be made that the price for modeling this outcome should be log-transformed.
- Advantages:
    - No houses would be predicted with negative sale prices
    - Errors in predicting expensive houses will not have an undue influence on the model.
    - From a statistical perspective, a logarithmic transform may also stabilize the variance in a way that makes inference more legitimate.
- Diadvantages:
    - Mostly relate to interpretation of model results
    - Units of the model coefficients might be more difficult to interpret, as will measures of performance
    - For example, the root mean squared error (RMSE) is a common performance metric used in regression models. It uses the difference between the observed and predicted values in its calculations. If the sale price is on the log scale, these differences (i.e., the residuals) are also on the log scale. It can be difficult to understand the quality of a model whose RMSE is 0.15 on such a log scale.

Regardless, we will use the log-transformed data.
 
```R
ggplot(ames, aes(x = Sale_Price)) + 
  geom_histogram(bins = 50, col= "white") +
  scale_x_log10()
ames <- ames %>% mutate(Sale_Price = log10(Sale_Price))
```

Runs through some exploratory data analysis on the spatial data and spots some potential
outliers / errors in the data.

Exploratory data analysis is critical prior to beginning any modelling as data characteristics
will shape about how the data should be processed and modeled. Some basic preliminary 
questions regarding this data would be:

- Is there anything odd or noticeable about the distributions of the individual predictors?
- Is there much skewness or any pathological distributions?
- Are there high correlations between predictors? For example, there are multiple predictors related to house size. Are some redundant?
- Are there associations between predictors and the outcomes?

5. Spending our data

Several steps to creating a useful model:

- Parameter estimation
- Model selection and tuning
- Performance assessment

At start of aproject there is an infinite pool of options, which we can refere to as the data budget. 

**Data spending**, how datya shiuld be applied to every task, is an important first consideration when
modelling as it relates to empirical validation.

When loads of data are available, a smart strat is to alloc specific subsets of data to 
different tasks (rather than use all data)to model something like parameter estimation. 
One strat when data and predictors are abundant is to spend specific subset of data to
determine which predictors are informative, before even considering parameter estimation. 
A solid methodlogy for data spending is important.

This chapter demonstrates the basics of splitting (i.e., creating a data budget) for our initial pool of samples for different purposes.

5.1 COMMON METHODS FOR SPLITTING DATA

Primray approach for empirical model validation is to split data into distinct sets:

- Training data: Majority of data used to develop and optimize the model and are a sandbox for
  model building where different models can be fit, feature engineering strategies are investigated etc.
- Test data: Tests efficacy of model(s) we have chosen. Critical to run test set only once otherwise it
  becomes part of the modelling process

At spliiting the data

